---
title: The Theory of Constraints - Part III
date: Sept 11, 2015
---

In case you missed the previous articles in this series, you can find [Part 1 here](http://www.higherorderheroku.com/articles/the-theory-of-constraints/) and [Part 2 here](http://www.example.com)

##Brief review.
In the two previous articles, I've drawn the analogy between factories and web applications, or more generally any piece of service-oriented software.  The reason this works is because we have to consider many of the same principles of optimization in a manufacturing context as we do with a piece of software.  We have to decide exactly what it is we wish to optimize, the goal if you will, and build functional assets that optimize and align with that goal.  In general, this process is iterative and ongoing.  The process of continuous improvement has been modeled several ways, one of my favorites is [Kanban](https://en.wikipedia.org/wiki/Kanban).  It is in this sense that we are continuously analyzing and optimization based on removing and pushing back system constraints or bottlenecks as they're often called.

##Key metrics.
Folks who run factories tend to be interested in three things: throughput, inventory and operational expense.  Throughput is the rate at which the system generates money through sales.  Inventory is all the money that the system has invested in purchasing things which it intends to sell.  And finally, operational expense is all the money the system spends in order to turn inventory into throughput.  While these definitions work perfectly fine for manufacturing, we need to recast them slightly to fit into the context of service-oriented software.  My proposed recasting looks something like this: Throughput is commonly understood as the number of requests a system can respond to in a given amount of time.  I would replace inventory with the term "operational assets" which simply means the physical or virtual resources that your application is using to process requests.  Think of these resources as servers, dynos, databases and external services.  Finally, operational expense turns into single request processing time.  This last metric can be a little tricky.  We could be talking about a single request on a cold system (cache not warmed up yet?), a mean time-to-process? or something like a 99th percentile, [Perc99](https://blog.heroku.com/archives/2014/2/3/heroku-xl#understanding-perc99-and-tail-latencies) for short.  Often called the "Goldilocks measurement" because it captures 99% of your apps performance, I tend to prefer Perc99 when evaluating request response times from service-oriented applications.

##How to allocate your assets.  Building the factory.
Now that we have a goal and a working vocabulary of important metrics, we can start to talk about how all these things fit together.  The goal piece is highly variable and is contextual based on individual interests.  There are however common goals that are nearly universal.  We talked about sub-second response times but I also mentioned optimized throughput in a previous installment of this series.  No matter what your optimization goal is, it is important to make sure you're optimizing to that goal.  If the goal that you've chosen changes, which is completely plausible given shifting business priorities, then what and how you optimize will certainly change.  As we start to assemble our system we need to make sure we measure performance against our stated goal.  With regard specifically to a service-oriented software application, we start by assembling resources that will serve as our "assembly line".  Specific to Heroku, we will lay out and size web dynos, worker dynos, databases and external services which maybe dependent services owned by you or third party services provisioned as [Heroku Add-ons](https://addons.heroku.com/).

##Size (and quantity) do matter
In the book "The Goal", Goldratt cautions against a factory that is running at 100% capacity all the time.  Something that's running at 100% is not efficient, and ultimately can't deal with variability.  Variability in this case is the natural ebb and flow of demand against a system.  If you're operating at a constant 100% capacity, you have no shock absorbers in the system.  The problem really boils down to statistical fluctuations and dependent events.  The best way to understand this is to realize that each process in the work flow is subject to statistical variations, that is the capacity for a given process can fluctuate around a statistical range.  The manifestation of this is that a bottleneck then will move within the system.  [A simple model for how this work can be found here.](http://maaw.info/MatchBowlExperiment.htm).  So the moral of the story here is that it is not simply a matter of tuning all the capacity for all of the processes in a system to the same level.  We'll often see a flood of long running database queries that bury a database for example, while leaving other system resources virtually idle.  In this case, the database is the bottleneck and needs to have it's capacity or efficiency increased so that it is no longer the bottleneck.

So going back to our factory analogy and thus considering bottlenecks, dependent events and statistical fluctuations, how can we use these concepts in terms of capacity planning for our service-oriented software?  The first thing that we need to consider is identifying the bottlenecks.  A bottleneck simply defined is any resource that is operating equal to or below demand.  Our job as engineers then is to identify those bottlenecks and increase the capacity at those bottlenecks.  Capacity can be increased by either increasing the size of the bounded resource or optimizing the code which runs inside the resource.  The choice often boils down to economics.  Bottleneck identification can be tricky, but we have tons of tools that help us do that.  As a remedy we can scale up or scale out things like dynos and databases or, like previously mentioned, we can optimize code that runs on these resources.  We can "outsource" our production processes by asynchronously off-loading processes that don't need to be done "right now".  This is software engineerings equivalent to a shock absorber.  The general pattern here is to queue up work that can be done by an out-of-band process.  We can do the same with databases by differentiating between read and write requests to which requests for on or the other get dispatched to a database designed specifically for read-only or write-update.  We can implement caches.  In a manufacturing context, this would be analogous to moving raw material closer to a machine that processes that material.  Lessen the physical distance to a machine, you reduce wait time and increase throughput.  The same analogy can be used with cached data.

## Conclusions
While these are general recommendations with regard to resource optimization, my real goal wasn't to provide a comprehensive tutorial but rather give a generalized framework, based on The Theory of Constraints that (hopefully) will give readers a fresh way to look at capacity planning.  Capacity and resource planning is often seen as a bit of a "black art", but it doesn't have to be.  By applying some scientific thought and rigor to the process, it can be rather strategic and intuitive.